<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>InternLM on ichigo-xin的博客</title>
        <link>https://ichigo-xin.github.io/categories/internlm/</link>
        <description>Recent content in InternLM on ichigo-xin的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 29 Jun 2024 10:22:20 +0800</lastBuildDate><atom:link href="https://ichigo-xin.github.io/categories/internlm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>书生·浦语大模型实战营第二期作业五</title>
        <link>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E4%BA%94/</link>
        <pubDate>Sat, 29 Jun 2024 10:22:20 +0800</pubDate>
        
        <guid>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E4%BA%94/</guid>
        <description>&lt;p&gt;LMDeploy部署实践截图&lt;/p&gt;
&lt;p&gt;环境准备&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-0.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;环境准备&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;下载依赖&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;下载依赖&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用Transformer库运行模型&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;使用Transformer库运行模型&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用LMDeploy与模型对话&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;使用LMDeploy与模型对话&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;设置最大KV Cache缓存大小&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;未设置&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;0.5&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;0.5&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;0.01&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-5-6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;0.01&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>书生·浦语大模型实战营第二期作业四</title>
        <link>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E5%9B%9B/</link>
        <pubDate>Tue, 25 Jun 2024 10:22:20 +0800</pubDate>
        
        <guid>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E5%9B%9B/</guid>
        <description>&lt;p&gt;训练自己的小助手认知复现&lt;/p&gt;
&lt;p&gt;环境准备&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-0.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;环境准备&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;下载依赖&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;下载依赖&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;训练数据准备&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;训练数据准备&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;训练模型准备&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;训练模型准备&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;训练准备完毕&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;训练准备完毕&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;训练结束&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;训练结束&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;测试微调模型&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-4-6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;测试微调模型&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>书生·浦语大模型实战营第二期作业三</title>
        <link>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E4%B8%89/</link>
        <pubDate>Mon, 24 Jun 2024 10:22:20 +0800</pubDate>
        
        <guid>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E4%B8%89/</guid>
        <description>&lt;h1 id=&#34;rag介绍&#34;&gt;RAG介绍&lt;/h1&gt;
&lt;p&gt;RAG（Retrieval Augmented Generation），字面意思就是 检索 - 增强 - 生成。通过检索向量库找到相关文本，再通过特定的prompt使用大模型的能力进行解答。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-1rag%e4%bb%8b%e7%bb%8d.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RAG原理&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;具体步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;先将文档知识加载。其中就可能就有ocr图片识别，同时对于长的文档还需要进行切分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;入向量库。一般是通过embedding模型将切分的文本片段进行向量化，加载到向量库中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;处理用户问题。用户输入问题时，将用户问题也通过embedding模型进行向量化，之后在通过余弦距离等找出相似的TopN个文本片段。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;组装prompt，用llm大模型进行回答。将上一步搜索出来的文本片段和用户问题组装好，输入给大模型，让大模型进行回答。
例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;文本信息内容如下. \n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;---------------------\n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;{context_str}&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;\n---------------------\n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;请使用上面的内容信息进行回答，而不是根据模型已有的知识，如果上面的内容信息没有答案，就回复不知道, &amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;回答这个问题: {query_str}\n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;rag可以解决以下问题：
1.大模型回答幻觉，模型对于不知道的知识可能会一本正经的胡说八道.
2.过时知识，因为模型训练的成本较高，不可能一直更新模型，对于近期的知识，模型是不知道的。使用rag技术可以先通过搜索将近期知识作为问题的一部分输入给模型，使用模型的能力对问题进行回答。
3.可以追溯回答来源。这一点在一些场景非常重要，对于问题回答的可溯源，增加可信的程度。&lt;/p&gt;
&lt;p&gt;RAG vs 微调&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-2ragvs%e5%be%ae%e8%b0%83.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RAG vs 微调&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于特定任务，即使通过prompt调整也不一定可以获得很好的效果，比如摘要总结这一个具体类型的任务，使用微调是更好的。
而对于结合知识进行回答的场景，RAG则有着它的优势。&lt;/p&gt;
&lt;h1 id=&#34;茴香豆&#34;&gt;茴香豆&lt;/h1&gt;
&lt;p&gt;茴香豆是一款可以嵌入到IM工具流中的RAG应用
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-4%e8%8c%b4%e9%a6%99%e8%b1%86%e5%b7%a5%e4%bd%9c%e6%b5%81.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;茴香豆工作流&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;工作流中有rejection pipeline，可以针对无意义的闲聊进行过滤。&lt;/p&gt;
&lt;h1 id=&#34;线上茴香豆助手对话截图&#34;&gt;线上茴香豆助手对话截图&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-4test1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;茴香豆助手对话截图&#34;
	
	
&gt;
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-4test2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;茴香豆助手对话截图&#34;
	
	
&gt;
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-4test3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;茴香豆助手对话截图&#34;
	
	
&gt;
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-4test4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;茴香豆助手对话截图&#34;
	
	
&gt;
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/InternLM-3-4test5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;茴香豆助手对话截图&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>书生·浦语大模型实战营第二期作业二</title>
        <link>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E4%BA%8C/</link>
        <pubDate>Thu, 20 Jun 2024 00:22:20 +0800</pubDate>
        
        <guid>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5%E7%AC%AC%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E4%BA%8C/</guid>
        <description>&lt;p&gt;使用 &lt;code&gt;InternLM2-Chat-1.8B&lt;/code&gt; 模型生成 300 字的小故事：
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/IntrnLM1-2-300%e5%ad%97%e5%b0%8f%e6%95%85%e4%ba%8b.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;300 字的小故事&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用书生·浦语 Web 和浦语对话，和书生·浦语对话，并找到书生·浦语 1 处表现不佳的案例：
&lt;img src=&#34;https://ichigo-xin.github.io/imgs/IntrnLM1-2-%e4%b8%8d%e4%bd%b3%e7%9a%84%e6%a1%88%e4%be%8b.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;不佳的案例&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>书生·浦语大模型开源开放体系</title>
        <link>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E4%BD%93%E7%B3%BB/</link>
        <pubDate>Wed, 19 Jun 2024 18:42:56 +0800</pubDate>
        
        <guid>https://ichigo-xin.github.io/p/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E4%BD%93%E7%B3%BB/</guid>
        <description>&lt;h1 id=&#34;大模型介绍&#34;&gt;大模型介绍&lt;/h1&gt;
&lt;p&gt;        首先介绍一下专用模型，专用模型是针对特定任务或特定领域设计和训练的模型。这些模型通常参数较少、结构较简单，但在特定任务上表现非常出色。比如在nlp领域，关键词提取可以使用jieba里面的TextRank算法，摘要提取可以使用Randeng-Pegasus-523M-Summary-Chinese，纠错可以使用ERNIE-CSC。在图像、语音等其他领域也有各种专用模型。&lt;/p&gt;
&lt;p&gt;        而大模型是具有大量参数、能够处理广泛任务的通用模型。这些模型在多个领域和任务上都表现出色，通常需要大量计算资源和数据进行训练。比如chatpdf中输入一篇文章，可以让它分析出关键词、摘要等。可以通过一个大模型解决多个问题。&lt;/p&gt;
&lt;p&gt;        大模型已经成为人工智能发展的最热门的词，自从ChatGPT问世以来，这一趋势更加明显。ChatGPT作为大规模语言模型的代表，以其卓越的自然语言理解和生成能力，迅速引起了广泛关注，并在多个领域展现了强大的应用潜力。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/IntrnLM1-1-%e9%80%9a%e7%94%a8%e6%a8%a1%e5%9e%8b.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;专用模型和大模型&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;书生浦语20介绍&#34;&gt;书生·浦语2.0介绍&lt;/h1&gt;
&lt;p&gt;        上海人工智能实验室与商汤科技联合香港中文大学和复旦大学正式发布新一代大语言模型书⽣·浦语2.0（InternLM2）。InternLM2 的核心理念在于回归语言建模的本质，致力于通过提高语料质量及信息密度，实现模型基座语言建模能力获得质的提升，进而在数理、代码、对话、创作等各方面都取得长足进步，综合性能达到同量级开源模型的领先水平。&lt;/p&gt;
&lt;p&gt;        InternLM2发布了7B和20B两个规格，同时每个规格分别提供InternLM2-Base、InternLM2和InternLM2-Chat三种模型版本。&lt;/p&gt;
&lt;p&gt;        InternLM2的主要亮点包括：超长上下文、综合性能提升、优秀的对话和创作体验、工具调用整体能力的提升、突出的数理能力和实用的数据分析功能。其中，超长上下文可以实现在20 万字长输入中几乎完美地实现长文“大海捞针”，配合代码解释器可以进行积分求解。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/IntrnLM1-1-%e4%ba%ae%e7%82%b9%e4%bb%8b%e7%bb%8d.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;专用模型和大模型&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;书生浦语开源开放体系&#34;&gt;书生·浦语开源开放体系&lt;/h1&gt;
&lt;p&gt;        书生·浦语开源开放体系包含数据、预训练、微调、部署、评测、应用这几个方面。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ichigo-xin.github.io/imgs/IntrnLM1-1-%e5%bc%80%e6%ba%90%e4%bd%93%e7%b3%bb.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;专用模型和大模型&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;        其中介绍我比较感兴趣的微调和部署这两个方面。&lt;/p&gt;
&lt;p&gt;        微调可以采用书生·浦语开放的高效微调框架 XTuner，XTuner支持加载HuggingFace、ModelScope 模型或数据集，支持训练多种模型，最低只需 8GB 显存即可微调 7B模型。微调分为增量续训和有监督微调。增量续训的用场景为让基座模型学习到一些新知识，如某个垂类领域知识，训练数据有文章、书籍、代码等。有监督微调的使用场景为让模型学会理解各种指令进行对话，或者注入少量领域知识，训练数据为高质量的对话、问答数据。&lt;/p&gt;
&lt;p&gt;        部署使用LMDeploy，LMDeploy提供大模型在GPU上部署的全流程解决方案，包括模型轻量化、推理和服务。可以实现模型的快速部署。
        &lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
